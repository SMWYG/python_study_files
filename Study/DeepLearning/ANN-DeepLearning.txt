=============================== ANN -> DeepLearning ===============================
■ 인공신경망의 도래
인간의 신경망을 본따 만든 것임 (컴퓨터가 학습을 하도록 만들고 싶은데 그럼 사람은 어떻게
학습을 하지? 라는 생각에서 나온 아이디어임)

1. 퍼셉트론(인간의 신경망에서 뉴런과 비슷한 느낌이라고 보면 됨)
    문제점 => 선형문제만 해결 가능 xor문제를 못품

2. 다층 퍼셉트론
    해결책 => 중간에 은닉층을 삽입해서 비선형 문제 해결
    문제점 => 은닉층 w, b의 학습이 불가능

3. 딥러닝_1
    해결책 => Backpropagation(Chain rule)으로 은닉층 w, b 학습 문제 해결
    문제점 => 은닉층이 많아지면 에러에 대한 미분값이 입력층까지 전달이 안됨(Vanishing Gradient) (문제는 sigmoid 함수에서 많은 영향을 받음)

4. 딥러닝_2
    해결책 => 1. 활성함수를 sigmoid함수가 아닌 ReLU함수로 대체(ReLU함수 이외에 다른 함수들(tanh, maxout, VLReLU 등)도 존재 이 문제를 해결하기 위한) (출력층의 활성함수는 Softmax를 사용함 여러 클래스의 최종 분류를 위해서)

이외에 학습을 잘하기 위해
◎ Learning rate 적절히 주기(너무 작거나 크지 않도록)
◎ 입력 데이터의 값들에 있어서 서로 너무 차이가 있지 않도록 전처리를 한다.(Standardization=Normalization)
◎ 초기값(w) 잘 주기 (RBM 사용 / 최근엔 RBM 사용 안함 => Xavier, LSUV 등)
◎ Overfiting(과적합) 문제의 해결을 위한 데이터수 늘리기, Regulalization(Dropout (학습할때만 사용(0.5~0.7 프로로) 평가나 실제 데이터에서는 keep_prob를 1 로 해줘야함))
◎ 앙상블(Ensemble) 사용
◎ Optimizer를 다른것을 써도됨
◎ Batch Normalizaion(배치 정규화)를 사용해도 됨 (Vanishing Gradient를 줄이기 위한 해결책 중 하나임)




*추가로 시간이 흐르면서 하드웨어의 발전도 학습을 더 빠르게 도와줌
===================================================================================



*은닉층의 개수, 은닉층 노드의 개수
=> 사용자 정의
*출력층 노드의 개수
=> 분류 클래스의 개수와 동일